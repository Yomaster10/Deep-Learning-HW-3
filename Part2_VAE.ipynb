{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bm}[1]{{\\bf #1}}\n",
    "\\newcommand{\\bb}[1]{\\bm{\\mathrm{#1}}}\n",
    "$$\n",
    "\n",
    "# Part 2: Variational Autoencoder\n",
    "<a id=part2></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this part we will learn to generate new data using a special type of autoencoder model which allows us to \n",
    "sample from its latent space. We'll implement and train a VAE and use it to generate new images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import unittest\n",
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import urllib\n",
    "import shutil\n",
    "import re\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test = unittest.TestCase()\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Obtaining the dataset\n",
    "<a id=part2_1></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's begin by downloading a dataset of images that we want to learn to generate. \n",
    "We'll use the [Labeled Faces in the Wild](http://vis-www.cs.umass.edu/lfw/) (LFW) dataset which contains many labeled faces of famous individuals.\n",
    "\n",
    "We're going to train our generative model to generate a specific face, not just any face.\n",
    "Since the person with the most images in this dataset is former president George W. Bush, we'll set out to train a Bush Generator :)\n",
    "\n",
    "However, if you feel adventurous and/or prefer to generate something else, feel free\n",
    "to edit the `PART2_CUSTOM_DATA_URL` variable in `hw3/answers.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import cs236781.plot as plot\n",
    "import cs236781.download\n",
    "from hw3.answers import PART2_CUSTOM_DATA_URL as CUSTOM_DATA_URL\n",
    "\n",
    "DATA_DIR = pathlib.Path.home().joinpath('.pytorch-datasets')\n",
    "if CUSTOM_DATA_URL is None:\n",
    "    DATA_URL = 'http://vis-www.cs.umass.edu/lfw/lfw-bush.zip'\n",
    "else:\n",
    "    DATA_URL = CUSTOM_DATA_URL\n",
    "\n",
    "_, dataset_dir = cs236781.download.download_data(out_path=DATA_DIR, url=DATA_URL, extract=True, force=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Create a `Dataset` object that will load the extraced images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "im_size = 64\n",
    "tf = T.Compose([\n",
    "    # Resize to constant spatial dimensions\n",
    "    T.Resize((im_size, im_size)),\n",
    "    # PIL.Image -> torch.Tensor\n",
    "    T.ToTensor(),\n",
    "    # Dynamic range [0,1] -> [-1, 1]\n",
    "    T.Normalize(mean=(.5,.5,.5), std=(.5,.5,.5)),\n",
    "])\n",
    "\n",
    "ds_gwb = ImageFolder(os.path.dirname(dataset_dir), tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "OK, let's see what we got. You can run the following block multiple times to display a random subset of images from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_ = plot.dataset_first_n(ds_gwb, 50, figsize=(15,10), nrows=5)\n",
    "print(f'Found {len(ds_gwb)} images in dataset folder.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x0, y0 = ds_gwb[0]\n",
    "x0 = x0.unsqueeze(0).to(device)\n",
    "print(x0.shape)\n",
    "\n",
    "test.assertSequenceEqual(x0.shape, (1, 3, im_size, im_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## The Variational Autoencoder\n",
    "<a id=part2_2></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "An autoencoder is a model which learns a representation of data in an **unsupervised** fashion (i.e without any labels).\n",
    "Recall it's general form from the lecture:\n",
    "\n",
    "<img src=\"imgs/autoencoder.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "An autoencoder maps an instance $\\bb{x}$ to a **latent-space** representation $\\bb{z}$.\n",
    "It has an encoder part, $\\Phi_{\\bb{\\alpha}}(\\bb{x})$ (a model with parameters $\\bb{\\alpha}$)\n",
    "and a decoder part, $\\Psi_{\\bb{\\beta}}(\\bb{z})$ (a model with parameters $\\bb{\\beta}$).\n",
    "\n",
    "While autoencoders can learn useful representations,\n",
    "generally it's hard to use them as generative models because there's no distribution we can sample from in the latent space. In other words, we have no way to choose a point $\\bb{z}$ in the latent space\n",
    "such that $\\Psi(\\bb{z})$ will end up on the data manifold in the instance space.\n",
    "\n",
    "<img src=\"imgs/ae_sampling.jpg\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The variational autoencoder (VAE), first proposed by [Kingma and Welling](https://arxiv.org/pdf/1312.6114.pdf), addresses this issue by taking a probabilistic perspective. \n",
    "Briefly, a VAE model can be described as follows.\n",
    "\n",
    "We define, in Baysean terminology,\n",
    "- The **prior** distribution $p(\\bb{Z})$ on points in the latent space.\n",
    "- The **posterior** distribution of points in the latent spaces given a specific instance: $p(\\bb{Z}|\\bb{X})$.\n",
    "- The **likelihood** distribution of a sample $\\bb{X}$ given a latent-space representation: $p(\\bb{X}|\\bb{Z})$.\n",
    "- The **evidence** distribution $p(\\bb{X})$ which is the distribution of the instance space due to the generative process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To create our variational **decoder** we'll further specify:\n",
    "\n",
    "- A parametric likelihood distribution, $p _{\\bb{\\beta}}(\\bb{X} | \\bb{Z}=\\bb{z}) = \\mathcal{N}( \\Psi _{\\bb{\\beta}}(\\bb{z}) , \\sigma^2 \\bb{I} )$. The interpretation is that given a latent $\\bb{z}$, we map it to a point normally distributed around the point calculated by our decoder neural network. Note that here $\\sigma^2$ is a hyperparameter while $\\vec{\\beta}$ represents the network parameters.\n",
    "- A fixed latent-space prior distribution of $p(\\bb{Z}) = \\mathcal{N}(\\bb{0},\\bb{I})$.\n",
    "\n",
    "This setting allows us to generate a new instance $\\bb{x}$ by sampling $\\bb{z}$ from the multivariate normal\n",
    "distribution, obtaining the instance-space mean $\\Psi _{\\bb{\\beta}}(\\bb{z})$ using our decoder network,\n",
    "and then sampling $\\bb{x}$ from $\\mathcal{N}( \\Psi _{\\bb{\\beta}}(\\bb{z}) , \\sigma^2 \\bb{I} )$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Our variational **encoder** will approximate the posterior with a parametric distribution \n",
    "$q _{\\bb{\\alpha}}(\\bb{Z} | \\bb{x}) =\n",
    "\\mathcal{N}( \\bb{\\mu} _{\\bb{\\alpha}}(\\bb{x}),  \\mathrm{diag}\\{ \\bb{\\sigma}^2_{\\bb{\\alpha}}(\\bb{x}) \\} )$.\n",
    "The interpretation is that our encoder model, $\\Phi_{\\vec{\\alpha}}(\\bb{x})$, calculates\n",
    "the mean and variance of the posterior distribution, and samples $\\bb{z}$ based on them.\n",
    "An important nuance here is that our network can't contain any stochastic elements that\n",
    "depend on the model parameters, otherwise we won't be able to back-propagate to those parameters.\n",
    "So sampling $\\bb{z}$ from $\\mathcal{N}( \\bb{\\mu} _{\\bb{\\alpha}}(\\bb{x}),  \\mathrm{diag}\\{ \\bb{\\sigma}^2_{\\bb{\\alpha}}(\\bb{x}) \\} )$ is not an option.\n",
    "The solution is to use what's known as the **reparametrization trick**: sample from an isotropic Gaussian, \n",
    "i.e. $\\bb{u}\\sim\\mathcal{N}(\\bb{0},\\bb{I})$ (which doesn't depend on trainable parameters), and calculate the latent representation as\n",
    "$\\bb{z} = \\bb{\\mu} _{\\bb{\\alpha}}(\\bb{x}) + \\bb{u}\\odot\\bb{\\sigma}_{\\bb{\\alpha}}(\\bb{x})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To train a VAE model, we maximize the evidence distribution, $p(\\bb{X})$ (see question below). \n",
    "The **VAE loss** can therefore be stated as minimizing $\\mathcal{L} = -\\mathbb{E}_{\\bb{x}} \\log p(\\bb{X})$.\n",
    "Although this expectation is intractable,\n",
    "we can obtain a lower-bound for $p(\\bb{X})$ (the evidence lower bound, \"ELBO\", shown in the lecture):\n",
    "\n",
    "$$\n",
    "\\log p(\\bb{X}) \\ge \\mathbb{E} _{\\bb{z} \\sim q _{\\bb{\\alpha}} }\\left[ \\log  p _{\\bb{\\beta}}(\\bb{X} | \\bb{z}) \\right]\n",
    "-  \\mathcal{D} _{\\mathrm{KL}}\\left(q _{\\bb{\\alpha}}(\\bb{Z} | \\bb{X})\\,\\left\\|\\, p(\\bb{Z} )\\right.\\right)\n",
    "$$\n",
    "\n",
    "where\n",
    "$\n",
    "\\mathcal{D} _{\\mathrm{KL}}(q\\left\\|\\right.p) =\n",
    "\\mathbb{E}_{\\bb{z}\\sim q}\\left[ \\log \\frac{q(\\bb{Z})}{p(\\bb{Z})} \\right]\n",
    "$\n",
    "is the Kullback-Liebler divergence, which can be interpreted as the information gained by using the posterior $q(\\bb{Z|X})$ instead of the prior distribution $p(\\bb{Z})$.\n",
    "\n",
    "Using the ELBO, the VAE loss becomes,\n",
    "$$\n",
    "\\mathcal{L}(\\vec{\\alpha},\\vec{\\beta}) = \\mathbb{E} _{\\bb{x}}  \\left[ \n",
    "\\mathbb{E} _{\\bb{z} \\sim q _{\\bb{\\alpha}} }\\left[ -\\log  p _{\\bb{\\beta}}(\\bb{x} | \\bb{z}) \\right]\n",
    "+  \\mathcal{D} _{\\mathrm{KL}}\\left(q _{\\bb{\\alpha}}(\\bb{Z} | \\bb{x})\\,\\left\\|\\, p(\\bb{Z} )\\right.\\right)\n",
    "\\right].\n",
    "$$\n",
    "\n",
    "By remembering that the likelihood is a Gaussian distribution with a diagonal covariance and by applying the reparametrization trick, we can write the above as\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\vec{\\alpha},\\vec{\\beta}) = \\mathbb{E} _{\\bb{x}}  \\left[ \n",
    "\\mathbb{E} _{\\bb{z} \\sim q _{\\bb{\\alpha}} }\n",
    "\\left[ \n",
    "\\frac{1}{2\\sigma^2}\\left\\| \\bb{x}- \\Psi _{\\bb{\\beta}}\\left(  \\bb{\\mu} _{\\bb{\\alpha}}(\\bb{x})  + \\bb{\\Sigma}^{\\frac{1}{2}} _{\\bb{\\alpha}}(\\bb{x}) \\bb{u}   \\right) \\right\\| _2^2\n",
    "\\right]\n",
    "+  \\mathcal{D} _{\\mathrm{KL}}\\left(q _{\\bb{\\alpha}}(\\bb{Z} | \\bb{x})\\,\\left\\|\\, p(\\bb{Z} )\\right.\\right)\n",
    "\\right].\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Model Implementation\n",
    "<a id=part2_3></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Obviously our model will have two parts, an encoder and a decoder.\n",
    "Since we're working with images, we'll implement both as deep **convolutional** networks, where the decoder is a \"mirror image\" of the encoder implemented with adjoint (AKA transposed) convolutions.\n",
    "Between the encoder CNN and the decoder CNN we'll implement the sampling from\n",
    "the parametric posterior approximator $q_{\\bb{\\alpha}}(\\bb{Z}|\\bb{x})$\n",
    "to make it a VAE model and not just a regular autoencoder (of course, this is not yet enough to create a VAE,\n",
    "since we also need a special loss function which we'll get to later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "First let's implement just the CNN part of the Encoder network\n",
    "(this is not the full $\\Phi_{\\vec{\\alpha}}(\\bb{x})$ yet).\n",
    "As usual, it should take an input image and map to a activation volume of a specified depth.\n",
    "We'll consider this volume as the features we extract from the input image.\n",
    "Later we'll use these to create the latent space representation of the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TODO**: Implement the `EncoderCNN` class in the `hw3/autoencoder.py` module.\n",
    "Implement any CNN architecture you like. If you need \"architecture inspiration\" you can see e.g. [this](https://arxiv.org/pdf/1512.09300.pdf) or [this](https://arxiv.org/pdf/1511.06434.pdf) paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import hw3.autoencoder as autoencoder\n",
    "\n",
    "in_channels = 3\n",
    "out_channels = 1024\n",
    "encoder_cnn = autoencoder.EncoderCNN(in_channels, out_channels).to(device)\n",
    "print(encoder_cnn)\n",
    "\n",
    "h = encoder_cnn(x0)\n",
    "print(h.shape)\n",
    "\n",
    "test.assertEqual(h.dim(), 4)\n",
    "test.assertSequenceEqual(h.shape[0:2], (1, out_channels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now let's implement the CNN part of the Decoder.\n",
    "Again this is not yet the full $\\Psi _{\\bb{\\beta}}(\\bb{z})$. It should take an activation volume produced\n",
    "by your `EncoderCNN` and output an image of the same dimensions as the Encoder's input was.\n",
    "This can be a CNN which is like a \"mirror image\" of the the Encoder. For example, replace convolutions with transposed convolutions, downsampling with up-sampling etc.\n",
    "Consult the documentation of [ConvTranspose2D](https://pytorch.org/docs/0.4.1/nn.html#convtranspose2d)\n",
    "to figure out how to reverse your convolutional layers in terms of input and output dimensions. Note that the decoder doesn't have to be exactly the opposite of the encoder and you can experiment with using a different architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TODO**: Implement the `DecoderCNN` class in the `hw3/autoencoder.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "decoder_cnn = autoencoder.DecoderCNN(in_channels=out_channels, out_channels=in_channels).to(device)\n",
    "print(decoder_cnn)\n",
    "x0r = decoder_cnn(h)\n",
    "print(x0r.shape)\n",
    "\n",
    "test.assertEqual(x0.shape, x0r.shape)\n",
    "\n",
    "# Should look like colored noise\n",
    "T.functional.to_pil_image(x0r[0].cpu().detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's now implement the full VAE Encoder, $\\Phi_{\\vec{\\alpha}}(\\vec{x})$.\n",
    "It will work as follows:\n",
    "1. Produce a feature vector $\\vec{h}$ from the input image $\\vec{x}$.\n",
    "2. Use two affine transforms to convert the features into the mean and log-variance of the posterior, i.e.\n",
    "    $$\n",
    "    \\begin{align}\n",
    "        \\bb{\\mu} _{\\bb{\\alpha}}(\\bb{x}) &= \\vec{h}\\mattr{W}_{\\mathrm{h\\mu}} + \\vec{b}_{\\mathrm{h\\mu}} \\\\\n",
    "        \\log\\left(\\bb{\\sigma}^2_{\\bb{\\alpha}}(\\bb{x})\\right) &= \\vec{h}\\mattr{W}_{\\mathrm{h\\sigma^2}} + \\vec{b}_{\\mathrm{h\\sigma^2}}\n",
    "    \\end{align}\n",
    "    $$\n",
    "3. Use the **reparametrization trick** to create the latent representation $\\vec{z}$.\n",
    "\n",
    "Notice that we model the **log** of the variance, not the actual variance.\n",
    "The above formulation is proposed in appendix C of the [VAE paper](https://arxiv.org/pdf/1312.6114.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TODO**: Implement the `encode()` method in the `VAE` class within the `hw3/autoencoder.py` module.\n",
    "You'll also need to define your parameters in `__init__()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "z_dim = 2\n",
    "vae = autoencoder.VAE(encoder_cnn, decoder_cnn, x0[0].size(), z_dim).to(device)\n",
    "print(vae)\n",
    "\n",
    "z, mu, log_sigma2 = vae.encode(x0)\n",
    "\n",
    "test.assertSequenceEqual(z.shape, (1, z_dim))\n",
    "test.assertTrue(z.shape == mu.shape == log_sigma2.shape)\n",
    "\n",
    "print(f'mu(x0)={list(*mu.detach().cpu().numpy())}, sigma2(x0)={list(*torch.exp(log_sigma2).detach().cpu().numpy())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's sample some 2d latent representations for an input image `x0` and visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Sample from q(Z|x)\n",
    "N = 500\n",
    "Z = torch.zeros(N, z_dim)\n",
    "_, ax = plt.subplots()\n",
    "with torch.no_grad():\n",
    "    for i in range(N):\n",
    "        Z[i], _, _ = vae.encode(x0)\n",
    "        ax.scatter(*Z[i].cpu().numpy())\n",
    "\n",
    "# Should be close to the mu/sigma in the previous block above\n",
    "print('sampled mu', torch.mean(Z, dim=0))\n",
    "print('sampled sigma2', torch.var(Z, dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's now implement the full VAE Decoder, $\\Psi _{\\bb{\\beta}}(\\bb{z})$.\n",
    "It will work as follows:\n",
    "1. Produce a feature vector $\\tilde{\\vec{h}}$ from the latent vector $\\vec{z}$ using an affine transform.\n",
    "2. Reconstruct an image $\\tilde{\\vec{x}}$ from $\\tilde{\\vec{h}}$ using the decoder CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TODO**: Implement the `decode()` method in the `VAE` class within the `hw3/autoencoder.py` module.\n",
    "You'll also need to define your parameters in `__init__()`. You may need to also re-run the block above after you implement this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x0r = vae.decode(z)\n",
    "\n",
    "test.assertSequenceEqual(x0r.shape, x0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Our model's `forward()` function will simply return `decode(encode(x))` as well as the calculated mean and log-variance of the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x0r, mu, log_sigma2 = vae(x0)\n",
    "\n",
    "test.assertSequenceEqual(x0r.shape, x0.shape)\n",
    "test.assertSequenceEqual(mu.shape, (1, z_dim))\n",
    "test.assertSequenceEqual(log_sigma2.shape, (1, z_dim))\n",
    "T.functional.to_pil_image(x0r[0].detach().cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Loss Implementation\n",
    "<a id=part2_4></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In practice, since we're using SGD, we'll drop the expectation over $\\bb{X}$ and instead sample an instance from the training set and compute a point-wise loss. Similarly, we'll drop the expectation over $\\bb{Z}$ by sampling from $q_{\\vec{\\alpha}}(\\bb{Z}|\\bb{x})$.\n",
    "Additionally, because the KL divergence is between two Gaussian distributions, there is a closed-form expression for it. These points bring us to the following point-wise loss:\n",
    "\n",
    "$$\n",
    "\\ell(\\vec{\\alpha},\\vec{\\beta};\\bb{x}) =\n",
    "\\frac{1}{\\sigma^2 d_x} \\left\\| \\bb{x}- \\Psi _{\\bb{\\beta}}\\left(  \\bb{\\mu} _{\\bb{\\alpha}}(\\bb{x})  +\n",
    "\\bb{\\Sigma}^{\\frac{1}{2}} _{\\bb{\\alpha}}(\\bb{x}) \\bb{u}   \\right) \\right\\| _2^2 +\n",
    "\\mathrm{tr}\\,\\bb{\\Sigma} _{\\bb{\\alpha}}(\\bb{x}) +  \\|\\bb{\\mu} _{\\bb{\\alpha}}(\\bb{x})\\|^2 _2 - d_z - \\log\\det \\bb{\\Sigma} _{\\bb{\\alpha}}(\\bb{x}),\n",
    "$$\n",
    "\n",
    "where $d_z$ is the dimension of the latent space, $d_x$ is the dimension of the input and $\\bb{u}\\sim\\mathcal{N}(\\bb{0},\\bb{I})$.\n",
    "This pointwise loss is the quantity that we'll compute and minimize with gradient descent.\n",
    "The first term corresponds to the data-reconstruction loss, while the second term corresponds to the KL-divergence loss.\n",
    "Note that the scaling by $d_x$ is not derived from the original loss formula and was added directly to the pointwise loss just to normalize the data term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TODO**: Implement the `vae_loss()` function in the `hw3/autoencoder.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from hw3.autoencoder import vae_loss\n",
    "torch.manual_seed(42)\n",
    "\n",
    "def test_vae_loss():\n",
    "    # Test data\n",
    "    N, C, H, W = 10, 3, 64, 64 \n",
    "    z_dim = 32\n",
    "    x  = torch.randn(N, C, H, W)*2 - 1\n",
    "    xr = torch.randn(N, C, H, W)*2 - 1\n",
    "    z_mu = torch.randn(N, z_dim)\n",
    "    z_log_sigma2 = torch.randn(N, z_dim)\n",
    "    x_sigma2 = 0.9\n",
    "    \n",
    "    loss, _, _ = vae_loss(x, xr, z_mu, z_log_sigma2, x_sigma2)\n",
    "    \n",
    "    test.assertAlmostEqual(loss.item(), 58.3234367, delta=1e-3)\n",
    "    return loss\n",
    "\n",
    "test_vae_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Sampling\n",
    "<a id=part2_5></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The main advantage of a VAE is that it can by used as a generative model by sampling the latent space, since\n",
    "we optimize for a isotropic Gaussian prior $p(\\bb{Z})$ in the loss function. Let's now implement this so that we can visualize how our model is doing when we train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TODO**: Implement the `sample()` method in the `VAE` class within the `hw3/autoencoder.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "samples = vae.sample(5)\n",
    "_ = plot.tensors_as_images(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Training\n",
    "<a id=part2_6></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Time to train!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TODO**:\n",
    "1. Implement the `VAETrainer` class in the `hw3/training.py` module. Make sure to implement the `checkpoints` feature of the `Trainer` class if you haven't done so already in Part 1.\n",
    "2. Tweak the hyperparameters in the `part2_vae_hyperparams()` function within the `hw3/answers.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import DataParallel\n",
    "from hw3.training import VAETrainer\n",
    "from hw3.answers import part2_vae_hyperparams\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Hyperparams\n",
    "hp = part2_vae_hyperparams()\n",
    "batch_size = hp['batch_size']\n",
    "h_dim = hp['h_dim']\n",
    "z_dim = hp['z_dim']\n",
    "x_sigma2 = hp['x_sigma2']\n",
    "learn_rate = hp['learn_rate']\n",
    "betas = hp['betas']\n",
    "\n",
    "# Data\n",
    "split_lengths = [int(len(ds_gwb)*0.9), int(len(ds_gwb)*0.1)]\n",
    "ds_train, ds_test = random_split(ds_gwb, split_lengths)\n",
    "dl_train = DataLoader(ds_train, batch_size, shuffle=True)\n",
    "dl_test  = DataLoader(ds_test,  batch_size, shuffle=True)\n",
    "im_size = ds_train[0][0].shape\n",
    "\n",
    "# Model\n",
    "encoder = autoencoder.EncoderCNN(in_channels=im_size[0], out_channels=h_dim)\n",
    "decoder = autoencoder.DecoderCNN(in_channels=h_dim, out_channels=im_size[0])\n",
    "vae = autoencoder.VAE(encoder, decoder, im_size, z_dim)\n",
    "vae_dp = DataParallel(vae).to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(vae.parameters(), lr=learn_rate, betas=betas)\n",
    "\n",
    "# Loss\n",
    "def loss_fn(x, xr, z_mu, z_log_sigma2):\n",
    "    return autoencoder.vae_loss(x, xr, z_mu, z_log_sigma2, x_sigma2)\n",
    "\n",
    "# Trainer\n",
    "trainer = VAETrainer(vae_dp, loss_fn, optimizer, device)\n",
    "checkpoint_file = 'checkpoints/vae'\n",
    "checkpoint_file_final = f'{checkpoint_file}_final'\n",
    "if os.path.isfile(f'{checkpoint_file}.pt'):\n",
    "    os.remove(f'{checkpoint_file}.pt')\n",
    "\n",
    "# Show model and hypers\n",
    "print(vae)\n",
    "print(hp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TODO**:\n",
    "1. Run the following block to train. It will sample some images from your model every few epochs so you can see the progress.\n",
    "2. When you're satisfied with your results, rename the checkpoints file by adding `_final`. When you run the `main.py` script to generate your submission, the final checkpoints file will be loaded instead of running training. Note that your final submission zip will not include the `checkpoints/` folder. This is OK.\n",
    "\n",
    "The images you get should be colorful, with different backgrounds and poses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import IPython.display\n",
    "\n",
    "def post_epoch_fn(epoch, train_result, test_result, verbose):\n",
    "    # Plot some samples if this is a verbose epoch\n",
    "    if verbose:\n",
    "        samples = vae.sample(n=5)\n",
    "        fig, _ = plot.tensors_as_images(samples, figsize=(6,2))\n",
    "        IPython.display.display(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "if os.path.isfile(f'{checkpoint_file_final}.pt'):\n",
    "    print(f'*** Loading final checkpoint file {checkpoint_file_final} instead of training')\n",
    "    checkpoint_file = checkpoint_file_final\n",
    "else:\n",
    "    res = trainer.fit(dl_train, dl_test,\n",
    "                      num_epochs=200, early_stopping=20, print_every=10,\n",
    "                      checkpoints=checkpoint_file,\n",
    "                      post_epoch_fn=post_epoch_fn)\n",
    "    \n",
    "# Plot images from best model\n",
    "saved_state = torch.load(f'{checkpoint_file}.pt', map_location=device)\n",
    "vae_dp.load_state_dict(saved_state['model_state'])\n",
    "print('*** Images Generated from best model:')\n",
    "fig, _ = plot.tensors_as_images(vae_dp.module.sample(n=15), nrows=3, figsize=(6,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Questions\n",
    "<a id=part2_7></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TODO** Answer the following questions. Write your answers in the appropriate variables in the module `hw3/answers.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from cs236781.answers import display_answer\n",
    "import hw3.answers as answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Question 1\n",
    "\n",
    "What does the $\\sigma^2$ hyperparameter (`x_sigma2` in the code) do? Explain the effect of low and high values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "display_answer(answers.part2_q1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Question 2\n",
    "\n",
    "1. Explain the purpose of both parts of the VAE loss term - reconstruction loss and KL divergence loss.\n",
    "2. How is the latent-space distribution affected by the KL loss term?\n",
    "3. What's the benefit of this effect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "display_answer(answers.part2_q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Question 3\n",
    "\n",
    "In the formulation of the VAE loss, why do we start by maximizing the evidence \n",
    "distribution, $p(\\bb{X})$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "display_answer(answers.part2_q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Question 4\n",
    "\n",
    "In the VAE encoder, why do we model the **log** of the \n",
    "latent-space variance corresponding to an input, $\\bb{\\sigma}^2_{\\bb{\\alpha}}$,\n",
    "instead of directly modelling this variance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_answer(answers.part2_q4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
